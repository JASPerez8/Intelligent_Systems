{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b481af9-3e7a-4407-8bde-8d01b5d3b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "# Load spaCy's English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56591d37-365b-4b44-b0a2-f00900104823",
   "metadata": {},
   "source": [
    "# Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72f3c3a-5ca9-4272-8d27-c5af9b0754c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "documents = [\n",
    "    \"The estate agent quickly marked out his territory on the dance floor.\",\n",
    "    \"The hummingbird's wings blurred while it eagerly sipped the sugar water from the feeder.\",\n",
    "    \"Jerry liked to look at paintings while eating garlic ice cream.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04ad12-95ea-4a0a-ac49-f4ee9fbaeee2",
   "metadata": {},
   "source": [
    "# Preprocess the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97f87a2-a9f4-4557-aef6-c921bda4e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize and lemmatize using spaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all documents\n",
    "processed_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Create a dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(processed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model1 = gensim.models.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=10)\n",
    "lda_model2 = gensim.models.LdaModel(corpus, num_topics=8, id2word=dictionary, passes=20)\n",
    "lda_model3 = gensim.models.LdaModel(corpus, num_topics=12, id2word=dictionary, passes=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05542fa2-2765-4327-bb81-27e6b2d629ef",
   "metadata": {},
   "source": [
    "# Print topics and their keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f567bec-dfdf-4dd8-80b3-169eb11aa978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.192*\"learning\" + 0.107*\"type\" + 0.106*\"machine\" + 0.106*\"supervise\" + '\n",
      "  '0.106*\"unsupervised\" + 0.106*\"main\" + 0.021*\"technique\" + '\n",
      "  '0.021*\"reinforcement\" + 0.021*\"improve\" + 0.021*\"algorithm\"'),\n",
      " (1,\n",
      "  '0.183*\"learning\" + 0.127*\"machine\" + 0.070*\"involve\" + '\n",
      "  '0.070*\"automatically\" + 0.070*\"experience\" + 0.070*\"algorithm\" + '\n",
      "  '0.070*\"improve\" + 0.070*\"reinforcement\" + 0.070*\"technique\" + 0.070*\"type\"'),\n",
      " (2,\n",
      "  '0.053*\"machine\" + 0.053*\"learning\" + 0.053*\"type\" + 0.053*\"technique\" + '\n",
      "  '0.053*\"reinforcement\" + 0.053*\"improve\" + 0.053*\"algorithm\" + '\n",
      "  '0.053*\"experience\" + 0.053*\"automatically\" + 0.053*\"involve\"'),\n",
      " (3,\n",
      "  '0.164*\"learning\" + 0.091*\"network\" + 0.091*\"subset\" + 0.091*\"deep\" + '\n",
      "  '0.091*\"base\" + 0.091*\"artificial\" + 0.091*\"neural\" + 0.091*\"machine\" + '\n",
      "  '0.018*\"type\" + 0.018*\"technique\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model1.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c210ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.120*\"machine\" + 0.120*\"algorithm\" + 0.120*\"experience\" + 0.120*\"improve\" '\n",
      "  '+ 0.120*\"involve\" + 0.120*\"automatically\" + 0.120*\"learning\" + 0.013*\"type\" '\n",
      "  '+ 0.013*\"supervise\" + 0.013*\"main\"'),\n",
      " (1,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"unsupervised\" + '\n",
      "  '0.053*\"supervise\" + 0.053*\"main\" + 0.053*\"subset\" + 0.053*\"network\" + '\n",
      "  '0.053*\"neural\" + 0.053*\"deep\"'),\n",
      " (2,\n",
      "  '0.224*\"learning\" + 0.116*\"machine\" + 0.061*\"network\" + 0.061*\"deep\" + '\n",
      "  '0.061*\"subset\" + 0.061*\"artificial\" + 0.061*\"base\" + 0.061*\"neural\" + '\n",
      "  '0.061*\"supervise\" + 0.061*\"main\"'),\n",
      " (3,\n",
      "  '0.254*\"learning\" + 0.134*\"type\" + 0.134*\"machine\" + 0.134*\"technique\" + '\n",
      "  '0.134*\"reinforcement\" + 0.015*\"subset\" + 0.015*\"unsupervised\" + '\n",
      "  '0.015*\"supervise\" + 0.015*\"main\" + 0.015*\"automatically\"'),\n",
      " (4,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"unsupervised\" + '\n",
      "  '0.053*\"main\" + 0.053*\"supervise\" + 0.053*\"subset\" + 0.053*\"deep\" + '\n",
      "  '0.053*\"base\" + 0.053*\"network\"'),\n",
      " (5,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"main\" + '\n",
      "  '0.053*\"unsupervised\" + 0.053*\"supervise\" + 0.053*\"deep\" + 0.053*\"neural\" + '\n",
      "  '0.053*\"subset\" + 0.053*\"base\"'),\n",
      " (6,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"main\" + '\n",
      "  '0.053*\"unsupervised\" + 0.053*\"supervise\" + 0.053*\"subset\" + 0.053*\"deep\" + '\n",
      "  '0.053*\"artificial\" + 0.053*\"network\"'),\n",
      " (7,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"supervise\" + '\n",
      "  '0.053*\"main\" + 0.053*\"unsupervised\" + 0.053*\"artificial\" + 0.053*\"subset\" + '\n",
      "  '0.053*\"neural\" + 0.053*\"deep\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model2.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b384ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.053*\"learning\" + 0.053*\"type\" + 0.053*\"machine\" + 0.053*\"network\" + '\n",
      "  '0.053*\"deep\" + 0.053*\"supervise\" + 0.053*\"technique\" + '\n",
      "  '0.053*\"reinforcement\" + 0.053*\"subset\" + 0.053*\"algorithm\"'),\n",
      " (1,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"neural\" + '\n",
      "  '0.053*\"deep\" + 0.053*\"supervise\" + 0.053*\"technique\" + '\n",
      "  '0.053*\"reinforcement\" + 0.053*\"subset\" + 0.053*\"algorithm\"'),\n",
      " (2,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"network\" + '\n",
      "  '0.053*\"deep\" + 0.053*\"supervise\" + 0.053*\"technique\" + '\n",
      "  '0.053*\"reinforcement\" + 0.053*\"algorithm\" + 0.053*\"automatically\"'),\n",
      " (3,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"neural\" + '\n",
      "  '0.053*\"deep\" + 0.053*\"supervise\" + 0.053*\"technique\" + '\n",
      "  '0.053*\"reinforcement\" + 0.053*\"subset\" + 0.053*\"algorithm\"'),\n",
      " (4,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"network\" + '\n",
      "  '0.053*\"deep\" + 0.053*\"supervise\" + 0.053*\"technique\" + '\n",
      "  '0.053*\"reinforcement\" + 0.053*\"subset\" + 0.053*\"algorithm\"'),\n",
      " (5,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"automatically\" + '\n",
      "  '0.053*\"unsupervised\" + 0.053*\"supervise\" + 0.053*\"type\" + 0.053*\"technique\" '\n",
      "  '+ 0.053*\"reinforcement\" + 0.053*\"algorithm\" + 0.053*\"experience\"'),\n",
      " (6,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"subset\" + '\n",
      "  '0.053*\"deep\" + 0.053*\"supervise\" + 0.053*\"main\" + 0.053*\"technique\" + '\n",
      "  '0.053*\"reinforcement\" + 0.053*\"algorithm\"'),\n",
      " (7,\n",
      "  '0.243*\"learning\" + 0.126*\"unsupervised\" + 0.126*\"supervise\" + 0.126*\"main\" '\n",
      "  '+ 0.126*\"type\" + 0.126*\"machine\" + 0.010*\"neural\" + 0.010*\"technique\" + '\n",
      "  '0.010*\"reinforcement\" + 0.010*\"subset\"'),\n",
      " (8,\n",
      "  '0.197*\"learning\" + 0.102*\"deep\" + 0.102*\"base\" + 0.102*\"artificial\" + '\n",
      "  '0.102*\"subset\" + 0.102*\"neural\" + 0.102*\"network\" + 0.102*\"machine\" + '\n",
      "  '0.008*\"type\" + 0.008*\"main\"'),\n",
      " (9,\n",
      "  '0.275*\"learning\" + 0.143*\"machine\" + 0.143*\"type\" + 0.143*\"technique\" + '\n",
      "  '0.143*\"reinforcement\" + 0.011*\"network\" + 0.011*\"deep\" + 0.011*\"supervise\" '\n",
      "  '+ 0.011*\"subset\" + 0.011*\"algorithm\"'),\n",
      " (10,\n",
      "  '0.126*\"experience\" + 0.126*\"improve\" + 0.126*\"involve\" + 0.126*\"algorithm\" '\n",
      "  '+ 0.126*\"machine\" + 0.126*\"automatically\" + 0.126*\"learning\" + 0.010*\"type\" '\n",
      "  '+ 0.010*\"reinforcement\" + 0.010*\"supervise\"'),\n",
      " (11,\n",
      "  '0.053*\"learning\" + 0.053*\"machine\" + 0.053*\"type\" + 0.053*\"subset\" + '\n",
      "  '0.053*\"deep\" + 0.053*\"supervise\" + 0.053*\"main\" + 0.053*\"technique\" + '\n",
      "  '0.053*\"reinforcement\" + 0.053*\"algorithm\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model3.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000de19a-8dd4-40e5-966b-25442d67ce78",
   "metadata": {},
   "source": [
    "# Assign topics to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da343547-03b4-41b6-b1b6-1faba58a87a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 - Topic: [(0, 0.031775404), (1, 0.90516657), (2, 0.031367213), (3, 0.031690866)]\n",
      "Document 2 - Topic: [(0, 0.025684685), (1, 0.025802366), (2, 0.02507629), (3, 0.92343664)]\n",
      "Document 3 - Topic: [(0, 0.03767094), (1, 0.88984954), (2, 0.03582462), (3, 0.036654897)]\n",
      "Document 4 - Topic: [(0, 0.904013), (1, 0.03262813), (2, 0.03132827), (3, 0.032030627)]\n"
     ]
    }
   ],
   "source": [
    "# Assign topics to documents\n",
    "for i, doc in enumerate(processed_documents):\n",
    "    print(f\"Document {i+1} - Topic: {lda_model1.get_document_topics(corpus[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d1027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 - Topic: [(0, 0.89058924), (1, 0.015625069), (2, 0.01564343), (3, 0.015641956), (4, 0.015625069), (5, 0.015625069), (6, 0.015625069), (7, 0.015625069)]\n",
      "Document 2 - Topic: [(0, 0.012504213), (1, 0.012500087), (2, 0.9124872), (3, 0.012508115), (4, 0.012500087), (5, 0.012500087), (6, 0.012500087), (7, 0.012500087)]\n",
      "Document 3 - Topic: [(0, 0.017866537), (1, 0.017857201), (2, 0.017878164), (3, 0.8749693), (4, 0.017857201), (5, 0.017857201), (6, 0.017857201), (7, 0.017857201)]\n",
      "Document 4 - Topic: [(0, 0.015631838), (1, 0.015625099), (2, 0.89059466), (3, 0.01564802), (4, 0.015625099), (5, 0.015625099), (6, 0.015625099), (7, 0.015625099)]\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(processed_documents):\n",
    "    print(f\"Document {i+1} - Topic: {lda_model2.get_document_topics(corpus[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b062d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 - Topic: [(0, 0.010416683), (1, 0.010416683), (2, 0.010416683), (3, 0.010416683), (4, 0.010416683), (5, 0.010416683), (6, 0.010416683), (7, 0.0104169445), (8, 0.010416893), (9, 0.010416982), (10, 0.88541573), (11, 0.010416683)]\n",
      "Document 2 - Topic: [(8, 0.9083327)]\n",
      "Document 3 - Topic: [(0, 0.011904774), (1, 0.011904774), (2, 0.011904774), (3, 0.011904774), (4, 0.011904774), (5, 0.011904774), (6, 0.011904774), (7, 0.011905126), (8, 0.011904987), (9, 0.86904675), (10, 0.011904932), (11, 0.011904774)]\n",
      "Document 4 - Topic: [(0, 0.010416679), (1, 0.010416679), (2, 0.010416679), (3, 0.010416679), (4, 0.010416679), (5, 0.010416679), (6, 0.010416679), (7, 0.8854158), (8, 0.010416858), (9, 0.010417089), (10, 0.010416813), (11, 0.010416679)]\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(processed_documents):\n",
    "    print(f\"Document {i+1} - Topic: {lda_model3.get_document_topics(corpus[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17631d05-4439-4d2d-a59a-fe43b98bf5f9",
   "metadata": {},
   "source": [
    "#                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
